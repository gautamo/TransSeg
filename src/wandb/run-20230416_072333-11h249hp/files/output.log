
Validation sanity check:   0% 0/2 [00:00<?, ?it/s]
  | Name      | Type          | Params
--------------------------------------------
0 | encoder   | BEiT3D        | 86.6 M
1 | decoder   | UPerHead      | 74.9 M
2 | criterion | DiceFocalLoss | 0
--------------------------------------------
161 M     Trainable params
0         Non-trainable params
161 M     Total params
646.244   Total estimated model params size (MB)
/usr/local/lib/python3.8/site-packages/torch/nn/functional.py:3060: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.
  warnings.warn("Default upsampling behavior when mode={} is changed "
/gdrive/MyDrive/TransSeg/src/utils.py:317: RuntimeWarning: invalid value encountered in true_divide
  acc = total_area_intersect / total_area_label
/gdrive/MyDrive/TransSeg/src/utils.py:321: RuntimeWarning: invalid value encountered in true_divide
  iou = total_area_intersect / total_area_union
/gdrive/MyDrive/TransSeg/src/utils.py:324: RuntimeWarning: invalid value encountered in true_divide
  dice = 2 * total_area_intersect / (total_area_pred_label + total_area_label)
Validation sanity check:   0% 0/2 [00:00<?, ?it/s][ 0.  0.  0.  0. nan nan  0.  0.]























































Epoch 0:  59% 1120/1889 [30:33<20:57,  1.64s/it, loss=0.816, v_num=49hp]








































 0.81425619 0.64506666]4 [10:09<00:00,  1.34it/s]
Epoch 0: 100% 1889/1889 [40:58<00:00,  1.30s/it, loss=0.85, v_num=49hp]
/usr/local/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.
























































Epoch 1:  59% 1120/1889 [30:33<20:57,  1.64s/it, loss=0.769, v_num=49hp]








































 0.84483799 0.71523528]4 [07:37<00:00,  1.72it/s]
























































Epoch 2:  59% 1120/1889 [30:30<20:55,  1.63s/it, loss=0.781, v_num=49hp]








































 0.80106784 0.73579209]4 [07:36<00:00,  1.72it/s]
























































Epoch 3:  59% 1120/1889 [30:31<20:56,  1.63s/it, loss=0.761, v_num=49hp]








































 0.80605051 0.74731161]4 [07:36<00:00,  1.72it/s]






























Epoch 4:  31% 580/1889 [16:01<36:06,  1.65s/it, loss=0.741, v_num=49hp]
/usr/local/lib/python3.8/site-packages/pytorch_lightning/core/datamodule.py:423: LightningDeprecationWarning: DataModule.prepare_data has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.prepare_data.
  rank_zero_deprecation(
Global seed set to 1234
Loading dataset:   2% 1/64 [00:00<00:19,  3.17it/s]

Loading dataset: 100% 64/64 [00:03<00:00, 20.52it/s]

Loading dataset: 100% 64/64 [00:03<00:00, 18.92it/s]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
# Train: 2210, # Val: 1568, # Test: 0...








































 0.80605051 0.74731161]4 [07:38<00:00,  1.72it/s]
--------------------------------------------------------------------------------
DATALOADER:0 VALIDATE RESULTS
{'val/acc': 0.9909312725067139,
 'val/ce_loss': 0.0,
 'val/dice_loss': 0.0,
 'val/loss': 0.7604896426200867,
 'val/macc': 0.7896319627761841,
 'val/mdice': 0.7402949929237366,
 'val/mdice_8': 0.8086086511611938,
 'val/mdice_nobg': 0.7206116914749146,
 'val/miou': 0.6172759532928467}
--------------------------------------------------------------------------------
Global seed set to 1234
Loading dataset:   2% 1/64 [00:00<00:23,  2.67it/s]

Loading dataset: 100% 64/64 [00:03<00:00, 20.78it/s]
Loading dataset: 100% 64/64 [00:03<00:00, 19.92it/s]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
# Train: 2210, # Val: 1568, # Test: 0...
Traceback (most recent call last):
  File "main.py", line 168, in <module>
    train(args)
  File "main.py", line 127, in train
    trainer.test(datamodule=dm)
  File "/usr/local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 705, in test
    results = self._run(model)
  File "/usr/local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 922, in _run
    self._dispatch()
  File "/usr/local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 986, in _dispatch
    self.accelerator.start_evaluating(self)
  File "/usr/local/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py", line 95, in start_evaluating
    self.training_type_plugin.start_evaluating(trainer)
  File "/usr/local/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py", line 165, in start_evaluating
    self._results = trainer.run_stage()
  File "/usr/local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 997, in run_stage
    return self._run_evaluate()
  File "/usr/local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1077, in _run_evaluate
    self._evaluation_loop.reload_evaluation_dataloaders()
  File "/usr/local/lib/python3.8/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py", line 170, in reload_evaluation_dataloaders
    self.trainer.reset_test_dataloader(model)
  File "/usr/local/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py", line 452, in reset_test_dataloader
    self.num_test_batches, self.test_dataloaders = self._reset_eval_dataloader(model, "test")
  File "/usr/local/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py", line 402, in _reset_eval_dataloader
    num_batches = len(dataloader) if has_len(dataloader) else float("inf")
  File "/usr/local/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py", line 63, in has_len
    raise ValueError("`Dataloader` returned 0 length. Please make sure that it returns at least 1 batch")
ValueError: `Dataloader` returned 0 length. Please make sure that it returns at least 1 batch