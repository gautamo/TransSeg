  | Name      | Type          | Params
--------------------------------------------
0 | encoder   | BEiT3D        | 86.6 M
1 | decoder   | UPerHead      | 74.9 M
2 | criterion | DiceFocalLoss | 0
--------------------------------------------
161 M     Trainable params
0         Non-trainable params
161 M     Total params
646.244   Total estimated model params size (MB)
/usr/local/lib/python3.8/site-packages/torch/nn/functional.py:3060: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.
  warnings.warn("Default upsampling behavior when mode={} is changed "
Validation sanity check:   0% 0/2 [00:00<?, ?it/s]
/gdrive/MyDrive/TransSeg/src/utils.py:317: RuntimeWarning: invalid value encountered in true_divide
  acc = total_area_intersect / total_area_label
/gdrive/MyDrive/TransSeg/src/utils.py:321: RuntimeWarning: invalid value encountered in true_divide
  iou = total_area_intersect / total_area_union
/gdrive/MyDrive/TransSeg/src/utils.py:324: RuntimeWarning: invalid value encountered in true_divide
  dice = 2 * total_area_intersect / (total_area_pred_label + total_area_label)
Validation sanity check:   0% 0/2 [00:00<?, ?it/s][ 0.  0.  0. nan  0. nan  0. nan]























































Epoch 0:  59% 1120/1889 [33:01<22:39,  1.77s/it, loss=0.837, v_num=w8c8]








































Validating: 100% 784/784 [12:33<00:00,  1.02s/it]
/usr/local/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.
 0.40399487 0.38721758]4 [12:33<00:00,  1.02s/it]
























































Epoch 1:  59% 1120/1889 [33:00<22:38,  1.77s/it, loss=0.783, v_num=w8c8]








































 0.76912957 0.66672874]4 [08:18<00:00,  1.57it/s]
























































Epoch 2:  59% 1120/1889 [32:59<22:37,  1.77s/it, loss=0.793, v_num=w8c8]








































 0.85288999 0.7777677 ]4 [08:18<00:00,  1.57it/s]
























































Epoch 3:  59% 1120/1889 [32:58<22:37,  1.76s/it, loss=0.771, v_num=w8c8]








































 0.86592177 0.76311303]4 [08:17<00:00,  1.58it/s]
























































Epoch 4:  59% 1120/1889 [32:59<22:37,  1.77s/it, loss=0.752, v_num=w8c8]








































 0.60532028 0.74593745]4 [08:18<00:00,  1.57it/s]























































Epoch 5:  59% 1120/1889 [32:59<22:37,  1.77s/it, loss=0.734, v_num=w8c8]








































 0.81612119 0.69375762]4 [08:18<00:00,  1.58it/s]
























































Epoch 6:  59% 1120/1889 [33:00<22:38,  1.77s/it, loss=0.716, v_num=w8c8]








































 0.87017429 0.8120175 ]4 [08:20<00:00,  1.57it/s]
























































Epoch 7:  59% 1120/1889 [32:59<22:37,  1.77s/it, loss=0.799, v_num=w8c8]








































 0.88568985 0.816759  ]4 [08:19<00:00,  1.57it/s]























































Epoch 8:  59% 1120/1889 [33:01<22:39,  1.77s/it, loss=0.775, v_num=w8c8]








































 0.8636277  0.78326516]4 [08:19<00:00,  1.57it/s]























































Epoch 9:  59% 1120/1889 [33:00<22:38,  1.77s/it, loss=0.734, v_num=w8c8]








































 0.89189257 0.81901852]4 [08:19<00:00,  1.57it/s]























































Epoch 10:  59% 1120/1889 [33:00<22:38,  1.77s/it, loss=0.746, v_num=w8c8]








































 0.86918966 0.77697029]4 [08:19<00:00,  1.57it/s]























